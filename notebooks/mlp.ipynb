{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import annotations\n",
    "from torchvision import datasets as D\n",
    "from torchvision import transforms as T\n",
    "import pathlib\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single example\n",
    "\n",
    "Suppose we have $L$ layer network. For the $i$-th node in the $l$-th layer, for training example $x^{(j)}$, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z^{[l](j)}_i&={w^{[l]}_i}^{T}a^{[l-1](j)}+b^{[l]}_i \\\\\n",
    "a^{[l](j)}_i&=g(z^{[l](j)}_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Or we can combine these two equations into one.\n",
    "\n",
    "$$\n",
    "a^{[l](j)}_i = g({w^{[l]}_i}^{T}a^{[l-1](j)}+b^{[l]}_i)\n",
    "$$\n",
    "\n",
    "Here, $g$ is the activation function, $l\\in[1,L],i\\in[1,n^{[l]}],j\\in[1,m]$, $L$ is the number of layers, $n^{[l]}$ is the number of nodes in layer $l$, $m$ is the number of training examples.\n",
    "\n",
    "We can stack all nodes in the same layer $l$ to have a system of equations.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{[l](j)}_1 &= g({w^{[l]}_1}^{T}a^{[l-1](j)}+b^{[l]}_1) \\\\\n",
    "a^{[l](j)}_2 &= g({w^{[l]}_2}^{T}a^{[l-1](j)}+b^{[l]}_2) \\\\\n",
    "\\vdots \\\\\n",
    "a^{[l](j)}_i &= g({w^{[l]}_i}^{T}a^{[l-1](j)}+b^{[l]}_i) \\\\\n",
    "\\vdots \\\\\n",
    "a^{[l](j)}_{n^{[l]}} &= g({w^{[l]}_{n^{[l]}}}^{T}a^{[l-1](j)}+b^{[l]}_{n^{[l]}})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can stack all activated values $a^{[l](j)}_i$ and get the system of equations below.\n",
    "\n",
    "$$\n",
    "a^{[l](j)}=g(\n",
    "\\begin{bmatrix}\n",
    "-{w^{[l]}_1}^T-\\\\\n",
    "-{w^{[l]}_2}^T-\\\\\n",
    "\\vdots\\\\\n",
    "-{w^{[l]}_i}^T-\\\\\n",
    "\\vdots\\\\\n",
    "-{w^{[l]}_{n^{[l]}}}^T-\n",
    "\\end{bmatrix}_{n^{[l]}\\times n^{[l-1]}}\n",
    "a^{[l-1](j)}+\n",
    "\\begin{bmatrix}\n",
    "b^{[l]}_1 \\\\\n",
    "b^{[l]}_2 \\\\\n",
    "\\vdots \\\\\n",
    "b^{[l]}_i \\\\\n",
    "\\vdots \\\\\n",
    "b^{[l]}_{n^{[l]}} \\\\\n",
    "\\end{bmatrix}_{n^{[l]}\\times 1}\n",
    ")\n",
    "$$\n",
    "\n",
    "For classification problem, we use $softmax$ function to convert the result from the output layer into valid probability distribution.\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(j)}=\n",
    "\\frac{1}{\\sum_{k=1}^{n^{L}}e^{a^{[L]}}_k}\n",
    "\\begin{bmatrix}\n",
    "e^{a^{[L]}}_1\\\\\n",
    "e^{a^{[L]}}_2\\\\\n",
    "\\vdots\\\\\n",
    "e^{a^{[L]}}_i\\\\\n",
    "\\vdots\\\\\n",
    "e^{a^{[L]}}_{n^{[L]}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally we can compute the cross-entropy loss $\\mathcal{L}$ for one example.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}^{(j)},y^{(j)})=-\\frac{1}{n^{[l]}}\\sum_{i=1}^{n^{[l]}}y_i^{(j)}\\ \\mathrm{ln}(\\hat{y}_i^{(j)})\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple examples\n",
    "\n",
    "We can stack the training examples in to one matrix like below.\n",
    "\n",
    "$$\n",
    "A^{[0]}=X=\n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "x^{(1)} & x^{(2)} & \\dots & x^{(m)} \\\\\n",
    "| & | & & | \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For every layer $l$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[l]}_{n^{[l]}\\times m}&=W^{[l]}_{n^{[l]}\\times n}A^{[l-1]}_{n^{[l-1]}\\times m}+b^{[l]}_{n^{[l]}\\times m} \\\\\n",
    "A^{[l]}&=\\sigma (Z^{[l]}).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Similarly, we have the MSE (Mean Squared Error)for for all examples.\n",
    "\n",
    "$$\n",
    "\\mathrm{L}(\\hat{y}, y)=\\frac{1}{n^{[L]}}\\sum_{j=1}^{n^{[L]}}\\mathcal{L}(\\hat{y}^{(j)},y^{(j)})\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Example\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "For any arbitrary node from sample $(j)$ in layer $[l]$ we have both $z^{[l](j)}$ and $a^{[l](j)}$. Since we know that from the previous section, that\n",
    "\n",
    "$$\n",
    "z_i^{[l](j)}=\\sum_{p=1}^{n^{[l-1]}}w_{ip}^{[l]}a_p^{[l](j)}+b_i^{[l]}.\n",
    "$$\n",
    "\n",
    "It's obvious that $\\frac{\\partial z_i^{[l](j)}}{\\partial w_{ip}^{[l](j)}}=a_p^{[l][j]},\\ p\\in[1,n^{[l-1]}]$. In the backward propagation, there is actually a \"forward pass\" inside.\n",
    "\n",
    "#### Backward Pass\n",
    "\n",
    "Since we can compute $\\frac{\\partial z_i^{[l](j)}}{\\partial w_{ip}^{[l](j)}}$ already, then the question is that how can we compute $\\frac{\\partial \\mathrm{L}}{\\partial z_i^{[l](j)}}$.\n",
    "\n",
    "We can also easily compute\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_i^{[l](j)}}{\\partial z_i^{[l](j)}}=g'(z_i^{[l](j)}).\n",
    "$$\n",
    "\n",
    "Then we have,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_i^{[l](j)}}\n",
    "    &=\\frac{\\partial \\mathcal{L}}{\\partial a_i^{[l](j)}}\\frac{\\partial a_i^{[l](j)}}{\\partial z_i^{[l](j)}}\\\\\n",
    "    &=\\frac{\\partial \\mathcal{L}}{\\partial a_i^{[l](j)}}g'(z_i^{[l](j)})\\\\\n",
    "    &=[\\sum_{k=1}^{n^{[l+1]}}\\frac{\\partial \\mathcal{L}}{\\partial z_k^{[l+1](j)}}\\frac{\\partial z_k^{[l+1](j)}}{\\partial a_i^{[l](j)}}]g'(z_i^{[l](j)})\\\\\n",
    "    &=[\\sum_{k=1}^{n^{[l+1]}}\\frac{\\partial \\mathcal{L}}{\\partial z_k^{[l+1](j)}}w_{ki}^{[l+1]}]g'(z_i^{[l](j)})\\\\\n",
    "    &={W_{\\_i}^{[l+1]}}^{T} \\frac{\\partial \\mathcal{L}}{\\partial z^{[l+1](j)}} g'(z_i^{[l](j)})\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here $W_{\\_i}$ is the $i$-th column of the matrix. We observe that there is a recursion appearing.\n",
    "\n",
    "We can let $\\delta_i^{[l](j)}=\\frac{\\partial \\mathcal{L}}{\\partial z_i^{[l](j)}}$ then we have,\n",
    "\n",
    "$$\n",
    "\\delta_i^{[l](j)}={W_{\\_i}^{[l+1]}}^{T}\\delta^{[l+1](j)}g'(z_i^{[l](j)})\\\\\n",
    "$$\n",
    "\n",
    "#### Putting It Together\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{ip}^{[l]}}\n",
    "    &=\\frac{\\partial \\mathcal{L}}{\\partial z_i^{[l+1](j)}}\\frac{\\partial z_i^{[l+1](j)}}{\\partial w_{ip}^{[l](j)}}\\\\\n",
    "    &=[\\sum_{k=1}^{n^{[l+1]}}[\\frac{\\partial \\mathcal{L}}{\\partial z_k^{[l+1](j)}}w_{ki}^{[l+1]}]g'(z_i^{[l](j)})]a_{p}^{[l]}\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_{i}^{[l]}}\n",
    "    &=\\frac{\\partial \\mathcal{L}}{\\partial z_i^{[l](j)}}\\frac{\\partial z_i^{[l+1](j)}}{\\partial w_{ip}^{[l](j)}}\\\\\n",
    "    &=\\sum_{k=1}^{n^{[l+1]}}[\\frac{\\partial \\mathcal{L}}{\\partial z_k^{[l+1](j)}}w_{ki}^{[l+1]}]g'(z_i^{[l](j)}).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can also vectorize it.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{W^{[l]}}\\mathrm{L}\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial \\mathrm{L}}{\\partial z_1^{[l](j)}}\\frac{\\partial z_1^{[l](j)}}{\\partial w_{11}^{[l](j)}} &\n",
    "    \\frac{\\partial \\mathrm{L}}{\\partial z_1^{[l](j)}}\\frac{\\partial z_1^{[l](j)}}{\\partial w_{12}^{[l](j)}} &\n",
    "    \\dots &\n",
    "    \\frac{\\partial \\mathrm{L}}{\\partial z_1^{[l](j)}}\\frac{\\partial z_1^{[l](j)}}{\\partial w_{1\\times n^{[l]}}^{[l](j)}} \\\\\n",
    "\n",
    "    \\frac{\\partial \\mathrm{L}}{\\partial z_2^{[l](j)}}\\frac{\\partial z_2^{[l](j)}}{\\partial w_{21}^{[l](j)}} &\n",
    "    \\frac{\\partial \\mathrm{L}}{\\partial z_2^{[l](j)}}\\frac{\\partial z_2^{[l](j)}}{\\partial w_{22}^{[l](j)}} &\n",
    "    \\dots &\n",
    "    \\frac{\\partial \\mathrm{L}}{\\partial z_2^{[l](j)}}\\frac{\\partial z_2^{[l](j)}}{\\partial w_{2\\times n^{[l]}}^{[l](j)}} \\\\\n",
    "\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\n",
    "    \\frac{\\partial \\mathrm{L}}{\\partial z_{n^{[l]}}^{[l](j)}}\\frac{\\partial z_{n^{[l]}}^{[l](j)}}{\\partial w_{n^{[l]}\\times 1}^{[l](j)}} &\n",
    "    \\frac{\\partial \\mathrm{L}}{\\partial z_{n^{[l]}}^{[l](j)}}\\frac{\\partial z_{n^{[l]}}^{[l](j)}}{\\partial w_{n^{[l]}\\times 2}^{[l](j)}} &\n",
    "    \\dots &\n",
    "    \\frac{\\partial \\mathrm{L}}{\\partial z_{n^{[l]}}^{[l](j)}}\\frac{\\partial z_{n^{[l]}}^{[l](j)}}{\\partial w_{n^{[l]}\\times n^{[l-1]}}^{[l](j)}} \\\\\n",
    "\n",
    "    \\end{bmatrix}, \\frac{\\partial \\mathrm{L}}{\\partial w_{ip}^{[l]}}=[\\sum_{k=1}^{n^{[l+1]}}[\\frac{\\partial \\mathrm{L}}{\\partial z_k^{[l+1](j)}}w_{ki}^{[l+1]}]g'(z_i^{[l](j)})]a_{p}^{[l]},p\\in[1,n^{[l-1]}],i\\in[1,n^{[l]}]\\\\\n",
    "    &=\n",
    "    \\nabla_{z^{[l+1](j)}}\\mathcal{L}\\ {a^{[l](j)}}^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Multiple Example\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "\\nabla_{Z^{[l]}}\\mathcal{L}&={W^{[l+1]}}^T\\nabla_{Z^{[l+1]}}\\mathcal{L}\\odot g'(Z^{[l]})\\\\\n",
    "\\nabla_{W^{[l]}}\\mathcal{L}&=\\frac{1}{m}\\nabla_{Z^{l]}}\\mathcal{L} {A^{[l-1]}}^T \\\\\n",
    "\\nabla_{b^{[l]}}\\mathcal{L}&=\\frac{1}{m}\\nabla_{Z^{[l]}}\\mathcal{L}\n",
    "    \\begin{bmatrix}\n",
    "        1 \\\\\n",
    "        \\vdots \\\\\n",
    "        1\n",
    "    \\end{bmatrix}\\Biggr\\}m\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Because of the transpose, the resulting dimension is $n^{[l]}\\times m$ and $m\\times n^{[l-1]}$ and $\\nabla_{W^{[l]}}\\mathrm{L}$ is still $n^{[l]}\\times n^{[l-1]}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Neural Networks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent works on every batch of the input data.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "W^{[l]}&:=W^{[l]}-\\eta\\nabla_{W^{[l]}}\\mathcal{L}(\\hat{y}-y) \\\\\n",
    "b^{[l]}&:=b^{[l]}-\\eta\\nabla_{b^{[l]}}\\mathcal{L}(\\hat{y}-y)\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MNIST dataset, we will use a network with two hidden layers.\n",
    "\n",
    "$$\n",
    "n^{[0]}=28\\times 28=784,n^{[1]}=128,n^{[2]}=64,n^{[3]}=10\n",
    "$$\n",
    "\n",
    "Cross-entropy function.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\hat{\\bold{Y}},\\bold{Y})&=-\\frac{1}{m}\\sum_{k=1}^{m}\\sum_{j=1}^{n^{[3]}}\\bold{y}_{j}^{(k)}\\mathrm{ln}(\\hat{\\bold{y}}_{j}^{(k)}) \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\bold{y}}_{j}^{(k)}}&=-\\frac{1}{m}\\cdot\\frac{\\bold{y}_{j}^{(k)}}{\\hat{\\bold{y}}_{j}^{(k)}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "$softmax$ function.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bold{a}^{[3](k)}\n",
    "    &= softmax(\\bold{z}^{[3](k)}) \\\\\n",
    "    &= \\frac{e^{\\bold{z}^{[3](k)}}}{\\sum_{i=1}^{n^{[3]}}e^{\\bold{z}_i^{[3](k)}}} \\\\\n",
    "    &= \\frac{e^{\\bold{z}^{[3](k)}}}{h} \\\\\n",
    "\\frac{\\partial \\bold{\\hat{y}}^{(k)}}{\\partial \\bold{a}^{[3](k)}}\n",
    "    &= \\frac{e^{\\bold{z}^{[3](k)}}h-e^{2\\bold{z}^{[3](k)}}}{h^2}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Back-propagation for layer ${[3]}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial \\bold{a}^{[3](k)}}\n",
    "&=\\frac{\\partial\\mathcal{L}}{\\partial\\hat{\\bold{y}}^{(k)}}\\frac{\\partial \\bold{\\hat{y}}^{(k)}}{\\partial \\bold{a}^{[3](k)}}\\\\\n",
    "\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial \\bold{z}^{[3](k)}}\n",
    "&=\\frac{\\partial\\mathcal{L}}{\\partial \\bold{a}^{[3](k)}}g'(\\bold{z}^{[3](k)})\\\\\n",
    "\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial W_{ij}^{[3]}}\n",
    "&=\\frac{\\partial\\mathcal{L}}{\\partial \\bold{z}_i^{[3](k)}}\\bold{a}_j^{[2](k)}\\\\\n",
    "\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial W^{[3]}}\n",
    "&=(\\frac{\\partial\\mathcal{L}}{\\partial \\bold{z}^{[3](k)}}){\\bold{a}^{[2](k)}}^T\\\\\n",
    "\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial \\bold{b}_{i}^{[3]}}\n",
    "&=\\frac{\\partial\\mathcal{L}}{\\partial \\bold{z}_i^{[3](k)}}\\\\\n",
    "\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial \\bold{b}^{[3]}}\n",
    "&=\\frac{\\partial\\mathcal{L}}{\\partial \\bold{z}^{[3](k)}}\\\\\n",
    "\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:   42000\n",
      "validation: 12000\n"
     ]
    }
   ],
   "source": [
    "dataset = D.MNIST(\n",
    "    root=pathlib.Path(\"/tmp/dataset\"), download=True, transform=T.ToTensor()\n",
    ")\n",
    "\n",
    "training_dataset, validation_dataset, test_dataset = random_split(\n",
    "    dataset, [0.7, 0.2, 0.1]\n",
    ")\n",
    "print(\"training:  \", len(training_dataset))\n",
    "print(\"validation:\", len(validation_dataset))\n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "samples = np.random.choice(len(dataset), 3)\n",
    "images = dataset.data[samples]\n",
    "labels = dataset.targets[samples]\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title(int(labels[0]))\n",
    "plt.imshow(images[0], cmap=\"gray\")\n",
    "plt.subplot(132)\n",
    "plt.imshow(images[1], cmap=\"gray\")\n",
    "plt.title(int(labels[1]))\n",
    "plt.subplot(133)\n",
    "plt.imshow(images[2], cmap=\"gray\")\n",
    "plt.title(int(labels[2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.mnist import MLPModel\n",
    "from modules.utils import relu, one_hot, softmax, cross_entropy, tanh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# --- Set up model\n",
    "#\n",
    "\n",
    "# Create model\n",
    "mlp = MLPModel([784, 128, 64, 10], num_classes=10)\n",
    "g = relu\n",
    "for i in range(len(mlp.sequential.layers)):\n",
    "    mlp.sequential.layers[i].g = g\n",
    "\n",
    "# Get first sample\n",
    "m = 1\n",
    "image, label = training_dataset[1]\n",
    "# Transform data to be trainable\n",
    "X = np.asarray(image).reshape(-1, 1)\n",
    "y = one_hot([label], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 49.84278055505847\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# --- Forward propagation ---\n",
    "#\n",
    "\n",
    "y_hat = mlp(X)\n",
    "loss = cross_entropy(y_hat, y)\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(mlp, learning_rate, y_hat, y, X0):\n",
    "    # Backward propagation\n",
    "    layers = mlp.sequential.layers\n",
    "    layer1 = layers[0]\n",
    "    layer2 = layers[1]\n",
    "    layer3 = layers[2]\n",
    "\n",
    "    A1 = layer1.cache[\"A\"]\n",
    "    A2 = layer2.cache[\"A\"]\n",
    "\n",
    "    Z1 = layer1.cache[\"Z\"]\n",
    "    Z2 = layer2.cache[\"Z\"]\n",
    "    Z3 = layer3.cache[\"Z\"]\n",
    "\n",
    "    d_Z3 = (\n",
    "        cross_entropy.dev(y_hat, y)  # dL/dY\n",
    "        * softmax.dev(y_hat)  # dY/dA3\n",
    "        * g.dev(Z3)  # dA3/dZ3\n",
    "        / layer3.cache[\"sigma\"]\n",
    "    )\n",
    "\n",
    "    d_W3 = 1 / m * d_Z3 @ A2.T\n",
    "    d_b3 = 1 / m * np.sum(d_Z3, axis=1, keepdims=1)\n",
    "\n",
    "    d_Z2 = layer3.W.T @ d_Z3 * g.dev(Z2) / layer2.cache[\"sigma\"]\n",
    "\n",
    "    d_W2 = 1 / m * d_Z2 @ A1.T\n",
    "    d_b2 = 1 / m * np.sum(d_Z2, axis=1, keepdims=1)\n",
    "\n",
    "    d_Z1 = layer2.W.T @ d_Z2 * g.dev(Z1) / layer1.cache[\"sigma\"]\n",
    "\n",
    "    d_W1 = 1 / m * d_Z1 @ X0.T\n",
    "    d_b1 = 1 / m * np.sum(d_Z1, axis=1, keepdims=1)\n",
    "\n",
    "    mlp.sequential.layers[0].W -= learning_rate * d_W1\n",
    "    mlp.sequential.layers[1].W -= learning_rate * d_W2\n",
    "    mlp.sequential.layers[2].W -= learning_rate * d_W3\n",
    "\n",
    "    mlp.sequential.layers[0].b -= learning_rate * d_b1\n",
    "    mlp.sequential.layers[1].b -= learning_rate * d_b2\n",
    "    mlp.sequential.layers[2].b -= learning_rate * d_b3\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.47855019 -0.37634103  0.65668372 ...  0.14169194 -0.4734928\n",
      "   0.70909664]\n",
      " [-1.14585023  1.84501246  0.19350625 ... -0.27626953  0.77626251\n",
      "   0.95285611]\n",
      " [ 1.01013746 -1.54307894 -0.60210779 ...  0.19118499  0.21881675\n",
      "  -0.80104442]\n",
      " ...\n",
      " [ 0.29174234  0.06672859  0.54843997 ... -0.69393808  0.01015568\n",
      "   1.48331388]\n",
      " [-1.63744866  0.17888241  0.26817919 ...  0.08209038  0.00514709\n",
      "  -0.7352581 ]\n",
      " [ 0.41890435 -0.00353325  2.27324453 ... -1.29854899 -0.9344092\n",
      "  -1.79356828]]\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPModel([784, 128, 64, 10], num_classes=10)\n",
    "g = relu\n",
    "print(mlp.sequential.layers[0].W)\n",
    "for i in range(len(mlp.sequential.layers)):\n",
    "    mlp.sequential.layers[i].g = g\n",
    "\n",
    "y_hat = mlp(X)\n",
    "mlp = backpropagation(mlp, 0.001, y_hat, y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 1\n",
    "mlp = MLPModel([784, 64, 32, 10], num_classes=10)\n",
    "\n",
    "# Set up activation function\n",
    "g = relu\n",
    "for i in range(len(mlp.sequential.layers)):\n",
    "    mlp.sequential.layers[i].g = g\n",
    "\n",
    "layers = mlp.sequential.layers\n",
    "m = training_loader.batch_size\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch, (image, label) in enumerate(training_loader):\n",
    "        X0 = image.numpy().reshape(784, len(label))\n",
    "        y = one_hot(label, 10)\n",
    "        # Forward propagation\n",
    "        y_hat = mlp(X0)\n",
    "        loss = cross_entropy(y_hat=y_hat, y=y)\n",
    "        running_loss += loss\n",
    "\n",
    "        mlp = backpropagation(mlp, learning_rate, y_hat, y, X0)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"epoch: {i}, batch: {batch:4d}, loss={loss:10.7f}\")\n",
    "\n",
    "    print(f\"loss={running_loss:7.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Activation Functions\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z)&=\\frac{1}{1+e^{-z}}\\\\\n",
    "g'(z)&=g(z)(1-g(z))=a(1-a)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### $tanh$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z)&=\\mathrm{tanh}(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}\\\\\n",
    "g'(z)&=1-\\mathrm{tanh}^2(z)\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### ReLU\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z)&=\\mathrm{max}(0,z)\\\\\n",
    "g'(z)&=\n",
    "\\begin{cases}\n",
    "0&if\\ z<0\\\\\n",
    "1&if\\ z>0\\\\\n",
    "undefined&if\\ z=0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Leaky ReLU\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z)&=\\mathrm{max}(0.01z,z)\\\\\n",
    "g'(z)&=\n",
    "\\begin{cases}\n",
    "0.01&if\\ z<0\\\\\n",
    "1&if\\ z\\geq0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Softmax\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "h(x)&=\\sum_{k=1}^{N}e^{x_k} \\\\\n",
    "f(x_j)&= \\frac{e^{x_j}}{h(x)} \\\\\n",
    "\\frac{\\partial f(x_j)}{\\partial x_j}&=\\frac{e^{x_j}h(x)-e^{2x_j}}{h^2(x)}\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### MSE Loss (L2-norm)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\hat{y}, y)&=\\frac{1}{2}\\sum_{k=1}^{N}||\\hat{y}-y||^2_2 \\\\\n",
    "\\frac{\\partial\\mathcal{L}(\\hat{y},y)}{\\partial \\hat{y}}&=\\hat{y}-y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}^{(j)},y^{(j)})=-\\frac{1}{N}\\sum_{i=1}^{N}y_i\\ \\mathrm{log}(\\hat{y}_i),\\ where \\sum y_i=1\\ and\\ \\sum\\hat{y_i}=1.\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
